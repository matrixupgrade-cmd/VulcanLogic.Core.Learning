/-!
===============================================================================
EpistemicGradients.lean â€” Sobolev-style Gradients for Probabilistic Attractor Hierarchies
===============================================================================
Author: Sean Timothy
Date: 2025-12-29
Purpose:
  Extend EpistemicVeil.lean with gradient-like operators on the probabilistic
  flow layer for finite-state, non-deterministic hierarchies.
  This sets the stage for sensitivity analysis / Sobolev-style reasoning.
===============================================================================
-/

import Mathlib.Probability.ProbabilityMassFunction
import Mathlib.Data.Finset.Basic
import Mathlib.Data.Real.Basic
import EpistemicVeil

open Finset ProbabilityMassFunction Real

variable {State : Type*} [Fintype State] [DecidableEq State] [Nonempty State]

-------------------------------------------------------------------------------
-- 0. Local gradient on PMF transitions
-------------------------------------------------------------------------------

/-- Local difference operator between two states in a probabilistic substrate -/
def local_gradient (P : ProbSubstrate State) (x y : State) : â„ :=
  let px := P.transition x
  let py := P.transition y
  âˆ‘ z in univ, |pmf.prob px z - pmf.prob py z|

/-- Normed gradient of expected reach probability wrt attractor -/
def reach_gradient (P : ProbSubstrate State) (steps : â„•) (x : State)
    (A : AttractorSpace (crisp_version P)) : â„ :=
  âˆ‘ y in A.carrier, |expected_reach_prob P steps x y - expected_reach_prob P steps x y|

-------------------------------------------------------------------------------
-- 1. Sobolev-style semi-norm (L2 variant)
-------------------------------------------------------------------------------

/-- L2 semi-norm across substrate states for a probabilistic layer -/
def L2_sobolev_norm (P : ProbSubstrate State) (steps : â„•)
    (A : AttractorSpace (crisp_version P)) : â„ :=
  sqrt (âˆ‘ x in univ, (reach_gradient P steps x A)^2)

-------------------------------------------------------------------------------
-- 2. Gradient propagation through nested hierarchy
-------------------------------------------------------------------------------

/-- Gradient at level n+1 from level n */
def hierarchy_gradient (P : ProbSubstrate State) (n : â„•)
    (A : NestedAttractor base_S (n+1)) : â„ :=
  âˆ‘ B in univ.filter (fun B : NestedAttractor base_S n => IsProbNested A B),
    L2_sobolev_norm P 1000 B.carrier  -- weighted sum over lower nested attractors

-------------------------------------------------------------------------------
-- 3. Sensitivity / perturbation operator
-------------------------------------------------------------------------------

/-- Small perturbation Î´ on transition probabilities, linearized effect */
def perturbation_effect (P : ProbSubstrate State) (Î´ : State â†’ State â†’ â„)
    (x : State) (A : AttractorSpace (crisp_version P)) : â„ :=
  âˆ‘ y in univ, Î´ x y * expected_reach_prob P 1000 x y

/-!
Interpretation / next steps:

â€¢ `local_gradient` â†’ measures sensitivity between two probabilistic state transitions.
â€¢ `reach_gradient` â†’ expected change in reach probability wrt attractor.
â€¢ `L2_sobolev_norm` â†’ Sobolev-style semi-norm for finite-state system.
â€¢ `hierarchy_gradient` â†’ propagates sensitivity across nested attractor levels.
â€¢ `perturbation_effect` â†’ linearized estimate of how small transition changes affect reach probabilities.

This gives a constructive, Lean-ready layer for superconductor-style gradient analysis
on top of the probabilistic epistemic veil.

Next: prove boundedness, monotonicity, and convergence of these norms, 
or link to sampling-based empirical estimates for finite-observer approximations. ğŸŒŒ
-/
